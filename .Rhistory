tunedModel <- control_stack_resamples() # If not tuning a model
## Define the model
lin_model <- linear_reg() %>%
set_engine("lm")
## Set up the whole workflow
linear_workflow <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(lin_model)
## Fit linear regression to folds
linreg_folds <- linear_workflow %>%
fit_resamples(resamples = folds, control = tunedModel)
## Penalized regression model
preg_model <- linear_reg(mixture=tune(), penalty=tune()) %>%
set_engine("glmnet")
preg_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(preg_model)
preg_tuning_grid <- grid_regular(mixture(), penalty(), levels = 5)
## fit folds
preg_folds_fit <- preg_wf %>%
tune_grid(resamples=folds, grid=preg_tuning_grid,
metrics=metric_set(rmse), control = untunedModel)
reg_tree <- decision_tree(tree_depth=tune(), cost_complexity=tune(), min_n=tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
## Workflow
regTree_wf <- workflow() %>%
add_recipe(bike_recipe) %>%
add_model(reg_tree)
## Tuning Grid
regTree_tuning_grid <- grid_regular(tree_depth(), cost_complexity(), min_n(), levels = 5)
## Tune the model
tree_folds_fit <- regTree_wf %>%
tune_grid(resamples = folds, grid = regTree_tuning_grid,
metrics = metric_set(rmse), control = untunedModel)
## Stack the models ##
bike_stack <- stacks() %>%
add_candidates(linreg_folds) %>%
add_candidates(preg_folds_fit) %>%
add_candidates(tree_folds_fit)
as_tibble(bike_stack)
stack_mod <- bike_stack %>%
blend_predictions() %>%
fit_members()
# Corrected parameter collection
collect_parameters(stack_mod, "tree_folds_fit")
stacked_preds <- predict(stack_mod, new_data = bikeTest) %>%
mutate(.pred = exp(.pred)) %>%
bind_cols(., bikeTest) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(datetime = as.character(format(datetime)))
stack_mod <- bike_stack %>%
blend_predictions() %>%
fit_members()
# Corrected parameter collection
collect_parameters(stack_mod, "tree_folds_fit")
bikeTest <- bikeTest %>%
mutate(year = factor(year(datetime)))
stacked_preds <- predict(stack_mod, new_data = bikeTest) %>%
mutate(.pred = exp(.pred)) %>%
bind_cols(., bikeTest) %>%
select(datetime, .pred) %>%
rename(count = .pred) %>%
mutate(datetime = as.character(format(datetime)))
## Write prediction file to CSV
vroom_write(x = stacked_preds, file = "./result.csv", delim = ",")
library(tidymodels)
library(embed)
## Read in the data
amazonTrain <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/train.csv")
amazonTest <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/test.csv")
## Cleaning & Feature Engineering
amazon_recipe <- recipe(rFormula, data=amazonTrain)) %>%
# apply the recipe to your data
prep <- prep(amazon_recipe)
library(tidymodels)
library(embed)
install.packages("embed")
## Read in the data
amazonTrain <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/train.csv")
amazonTest <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/test.csv")
## Cleaning & Feature Engineering
amazon_recipe <- recipe(rFormula, data=amazonTrain)) %>%
library(tidymodels)
library(embed)
## Read in the data
amazonTrain <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/train.csv")
amazonTest <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/test.csv")
## Cleaning & Feature Engineering
amazon_recipe <- recipe(rFormula, data=amazonTrain)) %>%
## Cleaning & Feature Engineering
amazon_recipe <- recipe(rFormula, data=amazonTrain) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(var2, threshold = .05) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(target_var)) #target encoding
## Cleaning & Feature Engineering
amazon_recipe <- recipe(ACTION~, data=amazonTrain) %>%
## Cleaning & Feature Engineering
amazon_recipe <- recipe(Action ~, data=amazonTrain) %>%
## Cleaning & Feature Engineering
amazon_recipe <- recipe(Action ~ , data=amazonTrain) %>%
## Cleaning & Feature Engineering
amazon_recipe <- recipe(Action ~  data=amazonTrain) %>%
## Cleaning & Feature Engineering
amazon_recipe <- recipe(Action ~ ., data = amazonTrain)%>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(var2, threshold = .05) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(target_var)) #target encoding
# apply the recipe to your data
prep <- prep(amazon_recipe)
## Cleaning & Feature Engineering
amazon_recipe <- recipe(Action ~ ., data = amazonTrain)%>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(var2, threshold = .05) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(target_var)) #target encoding
## Cleaning & Feature Engineering
amazon_recipe <- recipe(amazonTrain$Action ~ ., data = amazonTrain)%>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(var2, threshold = .05) %>% # combines categorical values that occur <5% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
step_lencode_mixed(all_nominal_predictors(), outcome = vars(target_var)) #target encoding
# apply the recipe to your data
prep <- prep(amazon_recipe)
library(tidymodels)
library(embed)
library(recipes)
## Read in the data
amazonTrain <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/train.csv")
amazonTest <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/test.csv")
## Cleaning & Feature Engineering
amazon_recipe <- recipe(Action ~ ., data = amazonTrain) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(var2, threshold = 0.05) %>%
step_dummy(all_nominal_predictors()) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(target_var))
## Cleaning & Feature Engineering
amazon_recipe <- recipe(ACTION ~ ., data = amazonTrain) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(var2, threshold = 0.05) %>%
step_dummy(all_nominal_predictors()) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(target_var))
# Now, bake the recipe to apply the transformations to your data
amazon_train_preprocessed <- bake(amazon_recipe, new_data = amazonTrain)
# Now, bake the recipe to apply the transformations to your data
amazon_train_preprocessed <- bake(amazon_recipe, new_data = amazonTrain)
## Cleaning & Feature Engineering
amazon_recipe <- recipe(ACTION ~ ., data = amazonTrain) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = 0.05) %>%
step_dummy(all_nominal_predictors()) %>%
# Now, bake the recipe to apply the transformations to your data
amazon_train_preprocessed <- bake(amazon_recipe, new_data = amazonTrain)
library(tidymodels)
library(embed)
library(recipes)
## Read in the data
amazonTrain <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/train.csv")
amazonTest <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/test.csv")
## Cleaning & Feature Engineering
amazon_recipe <- recipe(ACTION ~ ., data = amazonTrain) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = 0.05) %>%
step_dummy(all_nominal_predictors()) %>%
# apply the recipe to your data
prep <- prep(amazon_recipe)
baked <- bake(prep, new_data = amazonTrain)
library(tidymodels)
library(embed)
library(recipes)
## Read in the data
amazonTrain <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/train.csv")
amazonTest <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/test.csv")
az_recipe <- recipe(ACTION~., data=amazonTrain) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01)# combines categorical values that occur <1% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
# apply the recipe to your data
prep <- prep(az_recipe)
baked <- bake(prep, new_data = amazonTrain)
library(tidymodels)
library(embed)
library(recipes)
## Read in the data
amazonTrain <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/train.csv")
amazonTest <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/test.csv")
az_recipe <- recipe(ACTION~., data=amazonTrain) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
step_other(all_nominal_predictors(), threshold = .01)# combines categorical values that occur <1% into an "other" value
step_dummy(all_nominal_predictors()) %>% # dummy variable encoding
# apply the recipe to your data
prep <- prep(az_recipe)
library(tidymodels)
library(embed)
library(recipes)
## Read in the data
amazonTrain <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/train.csv")
amazonTest <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/test.csv")
az_recipe <- recipe(ACTION ~ ., data = amazonTrain) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = 0.01) %>%
step_dummy(all_nominal_predictors())
# Apply the recipe to your data and assign the result to 'prep'
prep <- prep(az_recipe)
# Bake the prepared recipe to transform the data
baked_train <- bake(prep, new_data = amazonTrain)
baked_test <- bake(prep, new_data = amazonTest)
baked_test <- bake(prep, new_data = amazonTest)
# Create a histogram for a numeric variable
ggplot(data = baked_train, aes(x = some_numeric_var)) +
geom_histogram(binwidth = 1, fill = "blue", color = "black") +
labs(title = "Histogram of Some Numeric Variable",
x = "Value",
y = "Frequency")
View(baked_train)
View(baked_train)
# Create a bar plot for a binary categorical variable
ggplot(data = baked_train, aes(x = ACTION)) +
geom_bar(fill = "blue", color = "black") +
labs(title = "Bar Plot of Action",
x = "Category",
y = "Count")
View(prep)
View(baked_train)
View(baked_test)
View(az_recipe)
###Logistic Regression###
my_mod <- logistic_reg() %>% #Type of model
set_engine("glm")
amazon_workflow <- workflow() %>%
add_recipe(az_recipe) %>%
add_model(my_mod) %>%
fit(data = amazonTrain) # Fit the workflow9
amazonTrain$ACTION <- factor(amazonTrain$ACTION)
az_recipe <- recipe(ACTION ~ ., data = amazonTrain) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = 0.01) %>%
step_dummy(all_nominal_predictors())
# Apply the recipe to your data and assign the result to 'prep'
prep <- prep(az_recipe)
# Bake the prepared recipe to transform the data
baked_train <- bake(prep, new_data = amazonTrain)
baked_test <- bake(prep, new_data = amazonTest)
###Logistic Regression###
my_mod <- logistic_reg() %>% #Type of model
set_engine("glm")
amazon_workflow <- workflow() %>%
add_recipe(az_recipe) %>%
add_model(my_mod) %>%
fit(data = amazonTrain) # Fit the workflow9
amazon_predictions <- predict(amazon_workflow,
new_data=amazonTest,
type=class) # "class" or "prob" (see doc)
amazon_predictions <- predict(amazon_workflow,
new_data = amazonTest,
type = "class")  # Use "class" for class labels
# View the first few rows of predictions
head(amazon_predictions)
## Write prediction file to CSV
vroom_write(x = amazon_predictions, file = "./amazon.csv", delim = ",")
amazon_predictions <- predict(amazon_workflow,
new_data = amazonTest,
type = "class")  # Use "class" for class labels
# Create a data frame for your submission
submission_df <- data.frame(Id = amazonTest$Id, Action = amazon_predictions)
submission_df <- data.frame(Id = amazonTest$Id, Action = amazon_predictions)
vroom_write(x = submission_df, file = "./amazon.csv", delim = ",")
amazon_predictions <- predict(amazon_workflow,
new_data = amazonTest,
type = "class")  # Use "class" for class labels
submission_df <- data.frame(Id = amazonTest$Id, Action = amazon_predictions)
baked_test <- bake(prep, new_data = amazonTest)
###Logistic Regression###
my_mod <- logistic_reg() %>% #Type of model
set_engine("glm")
amazon_workflow <- workflow() %>%
add_recipe(az_recipe) %>%
add_model(my_mod) %>%
fit(data = amazonTrain) # Fit the workflow9
amazon_predictions <- predict(amazon_workflow,
new_data = amazonTest,
type = "class")  # Use "class" for class labels
# Use vroom_write to write the data frame to a CSV file
# Create an 'Id' column with unique identifiers
amazonTest$Id <- 1:nrow(amazonTest)
submission_df <- data.frame(Id = amazonTest$Id, Action = amazon_predictions)
vroom_write(x = submission_df, file = "./amazon.csv", delim = ",")
# Use vroom_write to write the data frame to a CSV file
# Create an 'Id' column with unique identifiers
amazonTest$Id <- 1:nrow(amazonTest)
submission_df <- data.frame(Id = amazonTest$Id, Action = amazon_predictions)
names(submission_df)[names(submission_df) == "Action"] <- "Action"
vroom_write(x = submission_df, file = "./amazon.csv", delim = ",")
View(baked_train)
library(tidymodels)
library(embed)
library(recipes)
library(ggplot2)
## Read in the data
amazonTrain <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/train.csv")
amazonTest <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/test.csv")
amazonTrain$ACTION <- factor(amazonTrain$ACTION)
az_recipe <- recipe(ACTION ~ ., data = amazonTrain) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = 0.01) %>%
step_dummy(all_nominal_predictors())
# Apply the recipe to your data and assign the result to 'prep'
prep <- prep(az_recipe)
# Bake the prepared recipe to transform the data
baked_train <- bake(prep, new_data = amazonTrain)
baked_test <- bake(prep, new_data = amazonTest)
###Logistic Regression###
my_mod <- logistic_reg() %>% #Type of model
set_engine("glm")
amazon_workflow <- workflow() %>%
add_recipe(az_recipe) %>%
add_model(my_mod) %>%
fit(data = amazonTrain) # Fit the workflow9
amazon_predictions <- predict(amazon_workflow,
new_data = amazonTest,
type = "prob")  # Use "class" for class labels
# Use vroom_write to write the data frame to a CSV file
# Create an 'Id' column with unique identifiers
amazonTest$Id <- 1:nrow(amazonTest)
submission_df <- data.frame(Id = amazonTest$Id, Action = amazon_predictions)
names(submission_df)[names(submission_df) == "Action"] <- "Action"
vroom_write(x = submission_df, file = "./amazon.csv", delim = ",")
View(baked_train)
amazon_predictions
amazon_predictions <- predict(amazon_workflow,
new_data = amazonTest,
type = "prob") %>%
mutate(ACTION = ifelse(.pred_1 > .5, 1, 0))# Use "class" for class labels
amazon_predictions
amazon_predictions <- predict(amazon_workflow,
new_data = amazonTest,
type = "prob") %>%
transmute(ACTION = ifelse(.pred_1 > .5, 1, 0))# Use "class" for class labels
amazon_predictions
# Use vroom_write to write the data frame to a CSV file
# Create an 'Id' column with unique identifiers
amazonTest$Id <- 1:nrow(amazonTest)
submission_df <- cbind(amazonTest$id,amazon_predictions)
submission_df
View(az_recipe)
View(my_mod)
amazon_predictions <- predict(amazon_workflow,
new_data = amazonTest,
type = "prob") %>%
transmute(ACTION = ifelse(.pred_1 > .6, 1, 0))
# Use vroom_write to write the data frame to a CSV file
# Create an 'Id' column with unique identifiers
Id <- 1:nrow(amazonTest)
submission_df <- cbind(Id,amazon_predictions)
submission_df
vroom_write(x = submission_df, file = "./amazon.csv", delim = ",")
# Use vroom_write to write the data frame to a CSV file
# Create an 'Id' column with unique identifiers
Id <- 1:nrow(amazonTest)
submission_df <- cbind(Id,amazon_predictions)
vroom_write(x = submission_df, file = "./Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/amazon.csv", delim = ",")
# Use vroom_write to write the data frame to a CSV file
# Create an 'Id' column with unique identifiers
Id <- 1:nrow(amazonTest)
submission_df <- cbind(Id,amazon_predictions)
vroom_write(x = submission_df, file = "/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/amazon.csv", delim = ",")
amazon_predictions <- predict(amazon_workflow,
new_data = amazonTest,
type = "prob") %>%
transmute(ACTION = ifelse(.pred_1 > .75, 1, 0))
# Use vroom_write to write the data frame to a CSV file
# Create an 'Id' column with unique identifiers
Id <- 1:nrow(amazonTest)
submission_df <- cbind(Id,amazon_predictions)
vroom_write(x = submission_df, file = "/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/amazon.csv", delim = ",")
source("~/Desktop/STA348/KaggleBikeShare/BikeShareAnalysis.R")
library(tidymodels)
library(vroom)
library(tidymodels)
library(vroom)
# Read in the data
amazonTrain <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/train.csv")
amazonTest <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/test.csv")
# Convert ACTION to a factor
amazonTrain$ACTION <- factor(amazonTrain$ACTION)
# Create a recipe
az_recipe <- recipe(ACTION ~ ., data = amazonTrain) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_other(all_nominal_predictors(), threshold = 0.01) %>%
step_dummy(all_nominal_predictors())
# Define the random forest model
regres_mod <- rand_forest(mtry = tune(),
min_n = tune(),
trees = tune()) %>%
set_engine("ranger") %>%
set_mode("classification")
# Create a workflow with model & recipe
amazon_workflow <- workflow() %>%
add_recipe(az_recipe) %>%
add_model(regres_mod)
# Set up grid of tuning values
tuning_grid <- expand.grid(
mtry = c(2, 3, 4),    # Adjust the values you want to tune
min_n = c(5, 10, 20),  # Adjust the values you want to tune
trees = c(100, 250, 500)  # Adjust the values you want to tune
)
# Set up K-fold CV
folds <- vfold_cv(amazonTrain, v = 10, repeats = 1)  # Adjust K and repeats as needed
# Find best tuning parameters
CV_results <- amazon_workflow %>%
tune_grid(
resamples = folds,
grid = tuning_grid,
metrics = metric_set(roc_auc)
)
# Find the best tuning parameters based on ROC AUC
bestTune <- CV_results %>%
select_best("roc_auc")
# Finalize the workflow and fit it
final_wf <- amazon_workflow %>%
finalize_workflow(bestTune) %>%
fit(data = amazonTrain)
# Predict on new data
amazon_predictions <- predict(final_wf,
new_data = amazonTest,
type = "prob") %>%
transmute(ACTION = ifelse(.pred_1 > 0.75, 1, 0))  # Adjust threshold as needed
# Create an ID column
Id <- 1:nrow(amazonTest)
submission_df <- cbind(Id, amazon_predictions)
# Write the submission data frame to a CSV file
vroom_write(x = submission_df, file = "/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/amazon.csv", delim = ",")
# Predict on new data
amazon_predictions <- predict(final_wf,
new_data = amazonTest,
type = "prob")
# Create an ID column
Id <- 1:nrow(amazonTest)
submission_df <- cbind(Id, amazon_predictions)
# Write the submission data frame to a CSV file
vroom_write(x = submission_df, file = "/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/amazon.csv", delim = ",")
# Predict on new data
amazon_predictions <- predict(final_wf,
new_data = amazonTest,
type = "prob")
# Create an ID column
Id <- 1:nrow(amazonTest)
ACTION <-  amazon_predictions$.pred_1
submission_df <- cbind(Id, ACTION)
# Write the submission data frame to a CSV file
vroom_write(x = submission_df, file = "/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/amazon.csv", delim = ",")
View(submission_df)
# Write the submission data frame to a CSV file
vroom_write(x = submission_df, file = "/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/amazon.csv", delim = ",")
setwd("~/Desktop/STA348/AmazonEmployeeAccess")
# Write the submission data frame to a CSV file
vroom_write(x = submission_df, file = "./amazon.csv", delim = ",")
print(final_wf)
print(amazonTest)
# Predict on new data
amazon_predictions <- predict(final_wf,
new_data = amazonTest,
type = "prob")
# Check amazon_predictions
print(amazon_predictions)
View(amazonTest)
# Create an ID column
Id <- 1:nrow(amazonTest)
Action <-  amazon_predictions$.pred_1
submission_df <- data.frame(Id = Id, Action = Action)
View(submission_df)
# Write the submission data frame to a CSV file
vroom_write(x = submission_df, file = "./amazon.csv", delim = ",")
source("~/Desktop/STA348/KaggleBikeShare/BikeShareAnalysis.R")
library(tidymodels)
library(themis)
library(vroom)
library(embed)
library(rstanarm)
# Read in the data
amazonTrain <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/train.csv")
amazonTest <- vroom("/Users/cicizeng/Desktop/STA348/AmazonEmployeeAccess/test.csv")
# Convert ACTION to a factor
amazonTrain$ACTION <- factor(amazonTrain$ACTION)
# Create a recipe with target encoding
az_recipe <- recipe(ACTION ~ ., data = amazonTrain) %>%
step_mutate_at(all_numeric_predictors(), fn = factor) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
# Define the Random Forest model
rf_model <- rand_forest(mtry = tune(), trees = 1000, min_n = tune()) %>%
set_engine("randomForest") %>%
set_mode("classification")
# Create a workflow with the recipe and Random Forest model
rf_workflow <- workflow() %>%
add_recipe(az_recipe) %>%
add_model(rf_model)
# Set up grid of tuning values for Random Forest
tuning_grid_rf <- grid_regular(mtry(range = c(1,(ncol(amazonTrain)-1))),
min_n(),
levels = 10)
# Set up K-fold CV
folds <- vfold_cv(amazonTrain, v = 5, repeats = 1)
# Find best tuning parameters for Random Forest
CV_results_rf <- rf_workflow %>%
tune_grid(
resamples = folds,
grid = tuning_grid_rf,
metrics = metric_set(roc_auc)
)
